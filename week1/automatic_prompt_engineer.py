import os
from dotenv import load_dotenv
from ollama import chat
from typing import List, Dict, Optional
import json

load_dotenv()

MODEL_NAME = "qwen3:4b"
TEMPERATURE_GEN = 0.8   # æŒ‡ä»¤ç”Ÿæˆéœ€å¤šæ ·æ€§
TEMPERATURE_SCORE = 0.1 # è¯„åˆ†éœ€ç¡®å®šæ€§

'''
APE æ ¸å¿ƒæ€æƒ³ï¼ˆZhou et al., 2022ï¼‰
Goal: è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„ instructionï¼ˆå³ system/user promptï¼‰ï¼Œæ— éœ€äººå·¥è®¾è®¡ã€‚

æ ‡å‡†çš„ Automatic Prompt Engineer (APE) æµç¨‹æ˜¯ï¼š

1. ç”Ÿæˆå€™é€‰æŒ‡ä»¤ï¼ˆInstruction Generationï¼‰

2. è¯„ä¼°è¿™äº›æŒ‡ä»¤ï¼ˆInstruction Evaluation / Scoringï¼‰

3. é€‰æ‹©æœ€ä¼˜æŒ‡ä»¤ï¼Œå¹¶ç”¨å®ƒæ‰§è¡Œç›®æ ‡ä»»åŠ¡ï¼ˆExecution with Best Instructionï¼‰
'''

class AutomaticPromptEngineer:
    def __init__(
        self,
        target_model: str = MODEL_NAME,
        inference_model: Optional[str] = None,
        num_candidates: int = 5,
    ):
        self.target_model = target_model
        self.inference_model = inference_model or target_model
        self.num_candidates = num_candidates

    def _format_examples(self, examples: List[Dict[str, str]]) -> str:
        return "\n".join(
            f"Input: {ex['input']}\nOutput: {ex['output']}" for ex in examples
        )

    def generate_instructions(self, task_desc: str, examples: List[Dict[str, str]]) -> List[str]:
        demo = self._format_examples(examples)
        prompt = f"""You are an expert prompt engineer.
Given these input-output demonstrations:

{demo}

Write a clear, general instruction that would guide an AI to produce the correct output from any similar input.
Only output the instruction. No explanations.

Instruction:"""
        
        instructions = []
        for _ in range(self.num_candidates):
            resp = chat(model=self.inference_model, messages=[{"role": "user", "content": prompt}], 
                        options={"temperature": TEMPERATURE_GEN})
            instructions.append(resp.message.content.strip())
        return instructions

    def score_instruction(self, instruction: str, examples: List[Dict[str, str]]) -> float:
        demo = self._format_examples(examples)
        eval_prompt = f"""Evaluate this instruction:
"{instruction}"

Based on these demonstrations:
{demo}

Rate from 1 to 10 how well the instruction captures the task pattern (clarity, generality, alignment).

Respond ONLY: {{"score": <number>}}"""
        
        try:
            resp = chat(model=self.inference_model, messages=[{"role": "user", "content": eval_prompt}],
                        options={"temperature": TEMPERATURE_SCORE})
            score = float(json.loads(resp.message.content.strip()).get("score", 1))
            return max(1.0, min(10.0, score))
        except:
            return 1.0

    def execute_with_instruction(self, instruction: str, input_text: str) -> str:
        """âœ… ç¬¬ä¸‰é˜¶æ®µï¼šç”¨æœ€ä¼˜æŒ‡ä»¤æ‰§è¡Œæ–°è¾“å…¥"""
        resp = chat(
            model=self.target_model,
            messages=[
                {"role": "system", "content": instruction},
                {"role": "user", "content": input_text},
            ],
            options={"temperature": 0.0},  # deterministic
        )
        return resp.message.content.strip()

    def run_ape(
        self,
        task_description: str,
        examples: List[Dict[str, str]],
        test_input: str,  # ğŸ‘ˆ æ–°å¢ï¼šç”¨äºæœ€ç»ˆæ‰§è¡Œçš„è¾“å…¥
    ) -> Dict:
        """
        Full APE pipeline:
        1. Generate candidate instructions
        2. Score them (LLM-based, no ground truth)
        3. Select best and EXECUTE on test_input
        """
        print("Step 1: Generating candidate instructions...")
        candidates = self.generate_instructions(task_description, examples)

        print("Step 2: Scoring instructions...")
        scored = []
        for instr in candidates:
            score = self.score_instruction(instr, examples)
            scored.append({"instruction": instr, "score": score})
        scored.sort(key=lambda x: x["score"], reverse=True)

        best_instr = scored[0]["instruction"]
        print("Step 3: Executing best instruction on new input...")

        final_output = self.execute_with_instruction(best_instr, test_input)

        return {
            "task": task_description,
            "test_input": test_input,
            "best_instruction": best_instr,
            "final_output": final_output,
            "all_candidates": scored,
        }


# ==============================
# Example Usage
# ==============================

if __name__ == "__main__":
    TASK = "Reverse the letters in a word."
    EXAMPLES = [
        {"input": "cat", "output": "tac"},
        {"input": "dog", "output": "god"},
    ]
    TEST_INPUT = "httpstatus"  # â† æ–°è¾“å…¥ï¼Œç”¨äºæœ€ç»ˆæ‰§è¡Œ

    ape = AutomaticPromptEngineer(num_candidates=3)
    result = ape.run_ape(TASK, EXAMPLES, TEST_INPUT)

    print("\n" + "="*60)
    print("âœ… APE COMPLETE")
    print("="*60)
    print(f"Task: {result['task']}")
    print(f"Test Input: {result['test_input']}")
    print(f"\nBest Instruction:\n\"{result['best_instruction']}\"")
    print(f"\nFinal Output: {result['final_output']}")

    print("\nAll Candidates (scored):")
    for i, c in enumerate(result["all_candidates"], 1):
        print(f"  {i}. [{c['score']:.1f}/10] {c['instruction'][:60]}...")